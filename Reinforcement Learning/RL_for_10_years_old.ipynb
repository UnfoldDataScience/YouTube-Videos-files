{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHBAPkXvnwfG",
        "outputId": "b10f2ce3-d934-4f11-fb4d-6f38d3ac83aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the agent on the grid...\n",
            "Training completed.\n",
            "Starting demonstration...\n",
            "Current position: (0, 0)\n",
            "Selected move: 3\n",
            "Current position: (0, 1)\n",
            "Selected move: 1\n",
            "Current position: (1, 1)\n",
            "Selected move: 3\n",
            "Current position: (1, 2)\n",
            "Selected move: 3\n",
            "Current position: (1, 3)\n",
            "Selected move: 1\n",
            "Current position: (2, 3)\n",
            "Selected move: 1\n",
            "Current position: (3, 3)\n",
            "Selected move: 1\n",
            "Current position: (4, 3)\n",
            "Selected move: 3\n",
            "Target reached at position: (4, 4)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "class GridAgent:\n",
        "    #Define Agent and environemnt\n",
        "    def __init__(self, grid_size=5):\n",
        "        self.grid_size = grid_size\n",
        "        self.value_table = np.zeros((grid_size, grid_size, 4))  # Action-value table\n",
        "        self.explore_prob = 1.0  # Initial exploration probability\n",
        "        self.target = (grid_size - 1, grid_size - 1)  # Target position\n",
        "\n",
        "    #Initilaize the agent\n",
        "    def initialize_position(self):\n",
        "        \"\"\"Initialize the agent at the start of the grid.\"\"\"\n",
        "        self.position = (0, 0)\n",
        "        return self.position\n",
        "    #action\n",
        "    def take_action(self, move):\n",
        "        \"\"\"Update position based on the chosen move.\"\"\"\n",
        "        row, col = self.position\n",
        "        if move == 0:  # Move up\n",
        "            row = max(0, row - 1)\n",
        "        elif move == 1:  # Move down\n",
        "            row = min(self.grid_size - 1, row + 1)\n",
        "        elif move == 2:  # Move left\n",
        "            col = max(0, col - 1)\n",
        "        elif move == 3:  # Move right\n",
        "            col = min(self.grid_size - 1, col + 1)\n",
        "\n",
        "        self.position = (row, col)\n",
        "        reward = 1 if self.position == self.target else -1  # Reward\n",
        "        is_done = self.position == self.target\n",
        "        return self.position, reward, is_done\n",
        "\n",
        "    def select_move(self):\n",
        "        \"\"\"Select a move based on exploration or exploitation.\"\"\"\n",
        "        if np.random.rand() < self.explore_prob:\n",
        "            return np.random.randint(4)  # Explore randomly\n",
        "        return np.argmax(self.value_table[self.position])  # Exploit learned values\n",
        "\n",
        "    def learn(self, num_episodes=500):\n",
        "        \"\"\"Train the agent over a number of episodes.\"\"\"\n",
        "        for episode in range(num_episodes):\n",
        "            current_pos = self.initialize_position()\n",
        "            finished = False\n",
        "            while not finished:\n",
        "                move = self.select_move()\n",
        "                next_pos, reward, finished = self.take_action(move)\n",
        "                # Update the value table\n",
        "                best_future_val = np.max(self.value_table[next_pos])\n",
        "                self.value_table[current_pos][move] += 0.1 * (\n",
        "                    reward + 0.9 * best_future_val - self.value_table[current_pos][move]\n",
        "                )\n",
        "                current_pos = next_pos\n",
        "            # Reduce exploration probability gradually\n",
        "            self.explore_prob *= 0.99\n",
        "\n",
        "    def showcase(self):\n",
        "        \"\"\"Showcase the learned behavior of the agent.\"\"\"\n",
        "        current_pos = self.initialize_position()\n",
        "        reached_target = False\n",
        "        while not reached_target:\n",
        "            print(\"Current position:\", current_pos)\n",
        "            move = np.argmax(self.value_table[current_pos])  # Always exploit\n",
        "            print(\"Selected move:\", move)\n",
        "            current_pos, _, reached_target = self.take_action(move)\n",
        "            time.sleep(0.5)\n",
        "        print(\"Target reached at position:\", current_pos)\n",
        "\n",
        "\n",
        "# Create and train the agent\n",
        "navigator = GridAgent()\n",
        "\n",
        "print(\"Training the agent on the grid...\")\n",
        "navigator.learn()\n",
        "print(\"Training completed.\")\n",
        "\n",
        "print(\"Starting demonstration...\")\n",
        "navigator.showcase()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZMqpLuWan-Mw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}