{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6e27d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc3bd56f76048bdbdcbd39f50bd5c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Optimizer:', index=1, options=('Gradient Descent', 'Momentum', 'NAG', 'Adâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983b087b88a64dbfa5ff200b8191d110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Function to calculate the derivative of a quadratic function\n",
    "def gradient(x):\n",
    "    return 2 * x\n",
    "\n",
    "# Optimizer functions\n",
    "def gradient_descent(learning_rate, num_iterations):\n",
    "    x = 10  # Starting point\n",
    "    path = [x]\n",
    "    for i in range(num_iterations):\n",
    "        x -= learning_rate * gradient(x)\n",
    "        path.append(x)\n",
    "    return path\n",
    "\n",
    "def momentum(learning_rate, momentum_coef, num_iterations):\n",
    "    x = 10  # Starting point\n",
    "    v = 0\n",
    "    path = [x]\n",
    "    for i in range(num_iterations):\n",
    "        v = momentum_coef * v - learning_rate * gradient(x)\n",
    "        x += v\n",
    "        path.append(x)\n",
    "    return path\n",
    "\n",
    "def nag(learning_rate, momentum_coef, num_iterations):\n",
    "    x = 10  # Starting point\n",
    "    v = 0\n",
    "    path = [x]\n",
    "    for i in range(num_iterations):\n",
    "        lookahead_x = x + momentum_coef * v\n",
    "        v = momentum_coef * v - learning_rate * gradient(lookahead_x)\n",
    "        x += v\n",
    "        path.append(x)\n",
    "    return path\n",
    "\n",
    "def adagrad(learning_rate, num_iterations):\n",
    "    x = 10  # Starting point\n",
    "    cache = 0\n",
    "    path = [x]\n",
    "    for i in range(num_iterations):\n",
    "        g = gradient(x)\n",
    "        cache += g ** 2\n",
    "        adjusted_lr = learning_rate / (np.sqrt(cache) + 1e-8)\n",
    "        x -= adjusted_lr * g\n",
    "        path.append(x)\n",
    "    return path\n",
    "\n",
    "def rmsprop(learning_rate, decay_rate, num_iterations):\n",
    "    x = 10  # Starting point\n",
    "    cache = 0\n",
    "    path = [x]\n",
    "    for i in range(num_iterations):\n",
    "        g = gradient(x)\n",
    "        cache = decay_rate * cache + (1 - decay_rate) * g ** 2\n",
    "        adjusted_lr = learning_rate / (np.sqrt(cache) + 1e-8)\n",
    "        x -= adjusted_lr * g\n",
    "        path.append(x)\n",
    "    return path\n",
    "\n",
    "def adam(learning_rate, beta1, beta2, num_iterations):\n",
    "    x = 10  # Starting point\n",
    "    m, v = 0, 0\n",
    "    path = [x]\n",
    "    for i in range(num_iterations):\n",
    "        g = gradient(x)\n",
    "        m = beta1 * m + (1 - beta1) * g\n",
    "        v = beta2 * v + (1 - beta2) * g ** 2\n",
    "        m_hat = m / (1 - beta1 ** (i + 1))\n",
    "        v_hat = v / (1 - beta2 ** (i + 1))\n",
    "        x -= learning_rate * m_hat / (np.sqrt(v_hat) + 1e-8)\n",
    "        path.append(x)\n",
    "    return path\n",
    "\n",
    "# Plotting function\n",
    "def plot_convergence(optimizer, learning_rate, num_iterations, momentum_coef=0.9, decay_rate=0.9, beta1=0.9, beta2=0.999):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    if optimizer == \"Gradient Descent\":\n",
    "        path = gradient_descent(learning_rate, num_iterations)\n",
    "    elif optimizer == \"Momentum\":\n",
    "        path = momentum(learning_rate, momentum_coef, num_iterations)\n",
    "    elif optimizer == \"NAG\":\n",
    "        path = nag(learning_rate, momentum_coef, num_iterations)\n",
    "    elif optimizer == \"Adagrad\":\n",
    "        path = adagrad(learning_rate, num_iterations)\n",
    "    elif optimizer == \"RMSprop\":\n",
    "        path = rmsprop(learning_rate, decay_rate, num_iterations)\n",
    "    elif optimizer == \"Adam\":\n",
    "        path = adam(learning_rate, beta1, beta2, num_iterations)\n",
    "    \n",
    "    plt.plot(path, label=optimizer)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Parameter Value')\n",
    "    plt.title(f'{optimizer} Convergence')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive widgets\n",
    "optimizer_widget = widgets.Dropdown(\n",
    "    options=[\"Gradient Descent\", \"Momentum\", \"NAG\", \"Adagrad\", \"RMSprop\", \"Adam\"],\n",
    "    value=\"Momentum\",\n",
    "    description=\"Optimizer:\"\n",
    ")\n",
    "\n",
    "learning_rate_widget = widgets.FloatSlider(\n",
    "    value=0.1,\n",
    "    min=0.001,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description=\"Learning Rate:\"\n",
    ")\n",
    "\n",
    "num_iterations_widget = widgets.IntSlider(\n",
    "    value=100,\n",
    "    min=10,\n",
    "    max=500,\n",
    "    step=10,\n",
    "    description=\"Iterations:\"\n",
    ")\n",
    "\n",
    "momentum_widget = widgets.FloatSlider(\n",
    "    value=0.9,\n",
    "    min=0.5,\n",
    "    max=0.99,\n",
    "    step=0.01,\n",
    "    description=\"Momentum:\"\n",
    ")\n",
    "\n",
    "decay_rate_widget = widgets.FloatSlider(\n",
    "    value=0.9,\n",
    "    min=0.5,\n",
    "    max=0.99,\n",
    "    step=0.01,\n",
    "    description=\"Decay Rate:\"\n",
    ")\n",
    "\n",
    "beta1_widget = widgets.FloatSlider(\n",
    "    value=0.9,\n",
    "    min=0.5,\n",
    "    max=0.99,\n",
    "    step=0.01,\n",
    "    description=\"Beta 1:\"\n",
    ")\n",
    "\n",
    "beta2_widget = widgets.FloatSlider(\n",
    "    value=0.999,\n",
    "    min=0.5,\n",
    "    max=0.999,\n",
    "    step=0.01,\n",
    "    description=\"Beta 2:\"\n",
    ")\n",
    "\n",
    "ui = widgets.VBox([\n",
    "    optimizer_widget, \n",
    "    learning_rate_widget, \n",
    "    num_iterations_widget,\n",
    "    momentum_widget,\n",
    "    decay_rate_widget,\n",
    "    beta1_widget,\n",
    "    beta2_widget\n",
    "])\n",
    "\n",
    "out = widgets.interactive_output(\n",
    "    plot_convergence, \n",
    "    {\n",
    "        \"optimizer\": optimizer_widget, \n",
    "        \"learning_rate\": learning_rate_widget, \n",
    "        \"num_iterations\": num_iterations_widget,\n",
    "        \"momentum_coef\": momentum_widget,\n",
    "        \"decay_rate\": decay_rate_widget,\n",
    "        \"beta1\": beta1_widget,\n",
    "        \"beta2\": beta2_widget\n",
    "    }\n",
    ")\n",
    "\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c30b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to calculate the derivative of a quadratic function\n",
    "def gradient(x):\n",
    "    return x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6143c1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c7adf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
