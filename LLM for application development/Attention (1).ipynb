{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6DHeSYkhyf1t"
      },
      "outputs": [],
      "source": [
        "#1. Create Word Embedding\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "embeddings = {\n",
        "    \"The\": np.array([1.0, 0.0, 0.5]),\n",
        "    \"cat\": np.array([0.0, 1.0, 0.3]),\n",
        "    \"sat\": np.array([1.0, 1.0, 0.2])\n",
        "}\n",
        "\n",
        "# Create the input sequence as a matrix (shape: 3 x 2)\n",
        "X = np.stack([embeddings[\"The\"], embeddings[\"cat\"], embeddings[\"sat\"]])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYXT44K96inz",
        "outputId": "78cd3389-c0a3-444f-bd9f-f584f051f9f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1. , 0. , 0.5],\n",
              "       [0. , 1. , 0.3],\n",
              "       [1. , 1. , 0.2]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Create Q, K, V Matrices using Learnable Weights"
      ],
      "metadata": {
        "id": "KVCqx2b-6luP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulated weights (normally learned)\n",
        "W_Q = np.array([[1.0, 0.0, 0.5], [0.0, 1.0, 0.5],[0.0, 1.0, 0.5]])\n",
        "W_K = np.array([[1.0, 0.0, 0.5], [0.0, 1.0, 0.5],[0.0, 1.0, 0.5]])\n",
        "W_V = np.array([[0.5, 1.0, 0.2], [1.0, 0.5, 0.3],[1.0, 0.5, 0.1]])\n",
        "\n",
        "# Linear projections\n",
        "Q = X @ W_Q  # (3 x 3)\n",
        "K = X @ W_K  # (3 x 3)\n",
        "V = X @ W_V  # (3 x 3)"
      ],
      "metadata": {
        "id": "YMtsRYZV6stW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR86J8aR62XJ",
        "outputId": "fbdf4070-aa99-4b17-b831-d2171c2c5dbb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.  , 1.25, 0.25],\n",
              "       [1.3 , 0.65, 0.33],\n",
              "       [1.7 , 1.6 , 0.52]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmg79tMj63ZP",
        "outputId": "0868bcd7-2e53-4d59-a830-6c2d25398a1c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.  , 0.5 , 0.75],\n",
              "       [0.  , 1.3 , 0.65],\n",
              "       [1.  , 1.2 , 1.1 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Compute Dot Products Between Queries and Keys"
      ],
      "metadata": {
        "id": "mT94Fd_E6_ob"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = Q @ K.T  # (3 x 3)\n",
        "print(\"Dot product scores:\\n\", scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1_2H2kp8tgV",
        "outputId": "4503b7e6-5f30-4d70-895c-8ea02a4ae53b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dot product scores:\n",
            " [[1.8125 1.1375 2.425 ]\n",
            " [1.1375 2.1125 2.275 ]\n",
            " [2.425  2.275  3.65  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Scale by âˆšd_k"
      ],
      "metadata": {
        "id": "cSOfBghS80FW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dk = Q.shape[-1]\n",
        "scaled_scores = scores / np.sqrt(dk)"
      ],
      "metadata": {
        "id": "VZ_jfxHb85yP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKgq8h0-8-qQ",
        "outputId": "aaafd7de-ae7f-4588-c748-dd2a16ff881b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.04644736, 0.65673593, 1.4000744 ],\n",
              "       [0.65673593, 1.21965244, 1.31347186],\n",
              "       [1.4000744 , 1.31347186, 2.10732848]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Apply Softmax to Get Attention Weights"
      ],
      "metadata": {
        "id": "GGOf3WKq9HbE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))  # for stability\n",
        "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
        "\n",
        "attention_weights = softmax(scaled_scores)\n",
        "print(\"Attention Weights:\\n\", attention_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMB-Ptg49K8f",
        "outputId": "78f8d376-c4d4-4bbc-bacd-94fe258468f2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights:\n",
            " [[0.32242711 0.21836449 0.4592084 ]\n",
            " [0.21348029 0.37482567 0.41169404]\n",
            " [0.25345618 0.23242983 0.51411399]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Multiply by V to Get the Output"
      ],
      "metadata": {
        "id": "PItR76tN9QsQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = attention_weights @ V\n",
        "print(\"Final Output:\\n\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y817GiLI-MHh",
        "outputId": "6c9b3085-5964-49ee-dea9-eb0a34db9e12"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Output:\n",
            " [[1.38695523 1.27970424 0.39145543]\n",
            " [1.40063353 1.16919751 0.39114344]\n",
            " [1.42960874 1.290482   0.40740516]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
        "\n",
        "# Step 1: Word embeddings (3 tokens, each with 3-dim embedding)\n",
        "X = np.array([\n",
        "    [1.0, 0.0, 1.0],  # \"The\"\n",
        "    [0.0, 1.0, 1.0],  # \"cat\"\n",
        "    [1.0, 1.0, 0.0]   # \"sat\"\n",
        "])  # Shape: (3 tokens x 3 dim)\n",
        "\n",
        "# Step 2: Use identity matrices for Q, K, V projections (for clarity)\n",
        "Wq = np.eye(3)  # (3 x 3)\n",
        "Wk = np.eye(3)\n",
        "Wv = np.eye(3)\n",
        "\n",
        "Q = X @ Wq  # (3 x 3)\n",
        "K = X @ Wk  # (3 x 3)\n",
        "V = X @ Wv  # (3 x 3)\n",
        "\n",
        "# Step 3: Compute scaled attention scores\n",
        "scores = Q @ K.T  # (3 x 3)\n",
        "\n",
        "# Optional: scale by sqrt(d_k), d_k = 3\n",
        "dk = Q.shape[-1]\n",
        "scaled_scores = scores / np.sqrt(dk)\n",
        "\n",
        "# Step 4: Create causal mask (lower triangular, so we can't see the future)\n",
        "seq_len = X.shape[0]\n",
        "mask = np.triu(np.ones((seq_len, seq_len)) * -np.inf, k=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "qYFhMeZA-Nfr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply mask\n",
        "masked_scores = scaled_scores + mask"
      ],
      "metadata": {
        "id": "cYMxL2HICyra"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masked_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE3Dw5JKC-Qq",
        "outputId": "497b1c15-92ab-4abe-e671-961f2201ce23"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.15470054,       -inf,       -inf],\n",
              "       [0.57735027, 1.15470054,       -inf],\n",
              "       [0.57735027, 0.57735027, 1.15470054]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Softmax over masked scores\n",
        "attn_weights = softmax(masked_scores)\n",
        "attn_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCcdZlAGDA-l",
        "outputId": "327d8672-1a93-4c10-fdea-56e09aca3f91"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.        , 0.        ],\n",
              "       [0.35954252, 0.64045748, 0.        ],\n",
              "       [0.26445846, 0.26445846, 0.47108308]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Compute attention output\n",
        "output = attn_weights @ V  # (3 x 3)\n",
        "\n",
        "# Print everything\n",
        "print(\"Q:\\n\", Q)\n",
        "print(\"K:\\n\", K)\n",
        "print(\"V:\\n\", V)\n",
        "print(\"Masked Attention Weights:\\n\", attn_weights)\n",
        "print(\"Masked Self-Attention Output:\\n\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd5C4JQeCw-T",
        "outputId": "e3c71946-3a62-4008-a233-f26a9c23876f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q:\n",
            " [[1. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 1. 0.]]\n",
            "K:\n",
            " [[1. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 1. 0.]]\n",
            "V:\n",
            " [[1. 0. 1.]\n",
            " [0. 1. 1.]\n",
            " [1. 1. 0.]]\n",
            "Masked Attention Weights:\n",
            " [[1.         0.         0.        ]\n",
            " [0.35954252 0.64045748 0.        ]\n",
            " [0.26445846 0.26445846 0.47108308]]\n",
            "Masked Self-Attention Output:\n",
            " [[1.         0.         1.        ]\n",
            " [0.35954252 0.64045748 1.        ]\n",
            " [0.73554154 0.73554154 0.52891692]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SP8mu8snDE7g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}