{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ogzFio50OJjb",
    "outputId": "764ff3d9-6462-4655-b483-ab0149d45342"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.18.2)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install trl transformers datasets peft accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635
    },
    "id": "nLME6HkQkPKT",
    "outputId": "4edfa0a6-0c41-433d-dc72-688b7d963845"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-2744299785>:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.741100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.930700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.478100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.762200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.091700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.696000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.371700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.502300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.510300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.687400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.518000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.591300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.314100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.300800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.361000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15, training_loss=1.6571203509966532, metrics={'train_runtime': 3.4852, 'train_samples_per_second': 8.608, 'train_steps_per_second': 4.304, 'total_flos': 979526615040.0, 'train_loss': 1.6571203509966532, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "data = [\n",
    "    {\"prompt\": \"What is AI? \", \"chosen\": \"AI means Artificial Intelligence.\", \"rejected\": \"AI is a kind of fruit.\"},\n",
    "    {\"prompt\": \"Explain Python: \", \"chosen\": \"Python is a programming language.\", \"rejected\": \"Python is a type of snake.\"},\n",
    "    {\"prompt\": \"Describe climate change: \", \"chosen\": \"Climate change refers to long-term shifts in temperatures and weather patterns.\", \"rejected\": \"Climate change is a myth.\"},\n",
    "    {\"prompt\": \"What causes rain? \", \"chosen\": \"Rain is caused by water vapor condensing and falling from clouds.\", \"rejected\": \"Rain is caused by people crying.\"},\n",
    "    {\"prompt\": \"Define photosynthesis: \", \"chosen\": \"Photosynthesis is the process by which plants convert sunlight into energy.\", \"rejected\": \"Photosynthesis is a type of dance.\"},\n",
    "    {\"prompt\": \"What is reinforcement learning? \", \"chosen\": \"Reinforcement learning is a type of machine learning where agents learn by interacting with an environment.\", \"rejected\": \"Reinforcement learning is about reinforcing weak WiFi signals.\"},\n",
    "    {\"prompt\": \"What is quantum computing? \", \"chosen\": \"Quantum computing uses quantum bits to perform complex computations more efficiently.\", \"rejected\": \"Quantum computing is about computing how many quanta of light are in a room.\"},\n",
    "    {\"prompt\": \"Who wrote Hamlet? \", \"chosen\": \"Hamlet was written by William Shakespeare.\", \"rejected\": \"Hamlet is a famous soccer player.\"},\n",
    "    {\"prompt\": \"Explain gravity: \", \"chosen\": \"Gravity is the force that attracts two bodies towards each other.\", \"rejected\": \"Gravity is a made-up concept to explain why things fall.\"},\n",
    "    {\"prompt\": \"What is blockchain? \", \"chosen\": \"Blockchain is a decentralized digital ledger technology.\", \"rejected\": \"Blockchain is a chain made of blocks of stone.\"},\n",
    "]\n",
    "\n",
    "\n",
    "# Load tokenizer and model\n",
    "base_model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.train()\n",
    "\n",
    "# Format data for SFT\n",
    "sft_data = [\n",
    "    {\"input_ids\": tokenizer(d[\"prompt\"] + d[\"chosen\"], truncation=True, padding=\"max_length\", max_length=64, return_tensors=\"pt\")[\"input_ids\"].squeeze(), \"labels\": tokenizer(d[\"prompt\"] + d[\"chosen\"], truncation=True, padding=\"max_length\", max_length=64, return_tensors=\"pt\")[\"input_ids\"].squeeze()}\n",
    "    for d in data\n",
    "]\n",
    "\n",
    "class SFTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, idx): return self.data[idx]\n",
    "\n",
    "sft_dataset = SFTDataset(sft_data)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sft_model\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=sft_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B_V1AemvrbBP",
    "outputId": "a2bdd2a5-6d00-4e6f-a83b-458e3966f84d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./sft_finetuned_model/tokenizer_config.json',\n",
       " './sft_finetuned_model/special_tokens_map.json',\n",
       " './sft_finetuned_model/vocab.json',\n",
       " './sft_finetuned_model/merges.txt',\n",
       " './sft_finetuned_model/added_tokens.json',\n",
       " './sft_finetuned_model/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./sft_finetuned_model\")\n",
    "tokenizer.save_pretrained(\"./sft_finetuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pZN0wUlXkk1v",
    "outputId": "f7f56ab7-94d0-4281-d59e-239648097609"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "<ipython-input-19-1059347256>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  reward_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.408400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.039700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.599500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.154800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.808400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.953600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.307400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.218700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.313200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.046800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.049800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.042200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.020500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>12.566700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.492700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.022800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.228400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.445000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.911500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.601100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.333600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.279700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.080900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.025700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.582000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.299900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.971400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.043500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.020400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.755400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30, training_loss=1.4541328070064385, metrics={'train_runtime': 2.6384, 'train_samples_per_second': 22.741, 'train_steps_per_second': 11.37, 'total_flos': 1959070924800.0, 'train_loss': 1.4541328070064385, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# RM is a classifier scoring chosen > rejected\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=1)\n",
    "reward_model.resize_token_embeddings(len(tokenizer))\n",
    "reward_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "reward_model.train()\n",
    "\n",
    "# Prepare RM dataset\n",
    "rm_data = []\n",
    "for ex in data:\n",
    "    rm_data.append({\"text\": ex[\"prompt\"] + ex[\"chosen\"], \"label\": 1.0})\n",
    "    rm_data.append({\"text\": ex[\"prompt\"] + ex[\"rejected\"], \"label\": 0.0})\n",
    "\n",
    "class RMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = tokenizer(self.data[idx][\"text\"], truncation=True, padding=\"max_length\", max_length=64, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": torch.tensor(self.data[idx][\"label\"])\n",
    "        }\n",
    "\n",
    "rm_dataset = RMDataset(rm_data)\n",
    "\n",
    "reward_args = TrainingArguments(\n",
    "    output_dir=\"./rm_model\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "reward_trainer = Trainer(\n",
    "    model=reward_model,\n",
    "    args=reward_args,\n",
    "    train_dataset=rm_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "reward_trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1AfxBoywpYh"
   },
   "outputs": [],
   "source": [
    "reward_model.save_pretrained(\"./reward_model\")\n",
    "#tokenizer.save_pretrained(\"./sft_finetuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4XQtszBvmPIY",
    "outputId": "b9006e02-5c02-48b1-deca-c4648f70f2ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DPO] Epoch 0 | Loss: 0.6254\n",
      "[DPO] Epoch 0 | Loss: 0.7407\n",
      "[DPO] Epoch 0 | Loss: 0.6532\n",
      "[DPO] Epoch 0 | Loss: 0.6500\n",
      "[DPO] Epoch 0 | Loss: 0.5708\n",
      "[DPO] Epoch 0 | Loss: 0.7446\n",
      "[DPO] Epoch 0 | Loss: 0.6245\n",
      "[DPO] Epoch 0 | Loss: 0.6905\n",
      "[DPO] Epoch 0 | Loss: 0.6895\n",
      "[DPO] Epoch 0 | Loss: 0.6365\n",
      "[DPO] Epoch 1 | Loss: 0.6656\n",
      "[DPO] Epoch 1 | Loss: 0.6451\n",
      "[DPO] Epoch 1 | Loss: 0.6367\n",
      "[DPO] Epoch 1 | Loss: 0.6002\n",
      "[DPO] Epoch 1 | Loss: 0.5412\n",
      "[DPO] Epoch 1 | Loss: 0.5962\n",
      "[DPO] Epoch 1 | Loss: 0.5840\n",
      "[DPO] Epoch 1 | Loss: 0.6877\n",
      "[DPO] Epoch 1 | Loss: 0.6903\n",
      "[DPO] Epoch 1 | Loss: 0.5994\n",
      "[DPO] Epoch 2 | Loss: 0.6065\n",
      "[DPO] Epoch 2 | Loss: 0.5596\n",
      "[DPO] Epoch 2 | Loss: 0.6372\n",
      "[DPO] Epoch 2 | Loss: 0.5745\n",
      "[DPO] Epoch 2 | Loss: 0.5124\n",
      "[DPO] Epoch 2 | Loss: 0.6126\n",
      "[DPO] Epoch 2 | Loss: 0.5593\n",
      "[DPO] Epoch 2 | Loss: 0.6862\n",
      "[DPO] Epoch 2 | Loss: 0.6797\n",
      "[DPO] Epoch 2 | Loss: 0.5950\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./sft_finetuned_model\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.train()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
    "\n",
    "# DPO loss from earlier\n",
    "def dpo_loss(logits_chosen, logits_rejected, beta=0.1):\n",
    "    logp_chosen = torch.log_softmax(logits_chosen[:, -1, :], dim=-1)\n",
    "    logp_rejected = torch.log_softmax(logits_rejected[:, -1, :], dim=-1)\n",
    "    chosen_token_id = logits_chosen[:, -1, :].argmax(dim=-1)\n",
    "    rejected_token_id = logits_rejected[:, -1, :].argmax(dim=-1)\n",
    "    chosen_scores = logp_chosen.gather(1, chosen_token_id.unsqueeze(1))\n",
    "    rejected_scores = logp_rejected.gather(1, rejected_token_id.unsqueeze(1))\n",
    "    diff = chosen_scores - rejected_scores\n",
    "    return -torch.log(torch.sigmoid(beta * diff)).mean()\n",
    "\n",
    "# DPO Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    for example in data:\n",
    "        chosen_enc = tokenizer(example[\"prompt\"] + example[\"chosen\"], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=64)\n",
    "        rejected_enc = tokenizer(example[\"prompt\"] + example[\"rejected\"], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=64)\n",
    "        chosen_enc = {k: v.to(device) for k, v in chosen_enc.items()}\n",
    "        rejected_enc = {k: v.to(device) for k, v in rejected_enc.items()}\n",
    "        out_chosen = model(**chosen_enc)\n",
    "        out_rejected = model(**rejected_enc)\n",
    "        loss = dpo_loss(out_chosen.logits, out_rejected.logits)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"[DPO] Epoch {epoch} | Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "btwTllITnrmz",
    "outputId": "6957b573-c230-4c5e-c476-fe8bb089ced8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_dpo_model/tokenizer_config.json',\n",
       " './fine_tuned_dpo_model/special_tokens_map.json',\n",
       " './fine_tuned_dpo_model/vocab.json',\n",
       " './fine_tuned_dpo_model/merges.txt',\n",
       " './fine_tuned_dpo_model/added_tokens.json',\n",
       " './fine_tuned_dpo_model/tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./fine_tuned_dpo_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_dpo_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_4pD9xsmT3a",
    "outputId": "af29eaa4-0bd6-48a7-cacf-7db6eed1b041"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Base vs Fine-tuned Model Comparison:\n",
      "\n",
      "üß† Prompt: What is AI?\n",
      "   üîπ Base Model:       What is AI? AI is a computer-generated artificial intelligence.\n",
      "   üî∏ Fine-tuned Model: What is AI? AI is a computer vision technology.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Prompt to test\n",
    "test_prompts = [\n",
    "    \"What is AI?\"\n",
    "]\n",
    "\n",
    "# Load base model\n",
    "base_model_name = \"./sft_finetuned_model\"\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name).to(device)\n",
    "base_model.eval()\n",
    "\n",
    "# Fix padding if needed\n",
    "if base_tokenizer.pad_token is None:\n",
    "    base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "\n",
    "# Load fine-tuned model (from your DPO step)\n",
    "fine_tuned_path = \"./fine_tuned_dpo_model\"  # Change if you saved elsewhere\n",
    "ft_tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_dpo_model\")\n",
    "ft_model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_dpo_model\").to(device)\n",
    "ft_model.eval()\n",
    "\n",
    "# Function to generate response\n",
    "def generate_response(model, tokenizer, prompt, max_tokens=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Compare outputs\n",
    "print(\"\\nBase vs Fine-tuned Model Comparison:\\n\")\n",
    "for prompt in test_prompts:\n",
    "    base_response = generate_response(base_model, base_tokenizer, prompt)\n",
    "    ft_response = generate_response(ft_model, ft_tokenizer, prompt)\n",
    "\n",
    "    print(f\"   Prompt: {prompt}\")\n",
    "    print(f\"   üîπ Base Model:       {base_response}\")\n",
    "    print(f\"   üî∏ Fine-tuned Model: {ft_response}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 580,
     "referenced_widgets": [
      "126be89f06f84aec86a5d8484197f3a6",
      "2817a17fa0024d099b301c7a61569b51",
      "796340f619a049c48f1b06a53b93f46a",
      "d3266f870ebc4dd9bac9397e6de25b9f",
      "4c4ff5349dc84c2580dbbfe2e140e2c2",
      "13b7e420f485444eaf2448d57b8ec5b3",
      "806f3fe1a78e497a8076b1749679ecf1",
      "a116a775b8214364938c9aa2cc58e489",
      "83aa76a170904d4eb0470f1071dce97b",
      "c69c7beddb1541328337acc45010a859",
      "d8ea287e3d214279b4ecd50082fc2215",
      "fe66e69304ea4eb59e6c0ade939f4001",
      "946a6346ac414179ae10ba1f283dcbbe",
      "3d6067bddebf49fe98c8d65dd5e517ae",
      "b763addd8c804547a5ea80828e87d235",
      "16130d73dc0343f2913dc55e8160c7bd",
      "56eee0fc94814b1ba3084eb8f75ab1e4",
      "195119c9ec7447b6bfe1e9835110a003",
      "755a3e22530d44a0ab822b95fe5cb8be",
      "bfa8eaf63dae4e0d86fbb47e2a6e1ec7",
      "e075e20d0cff4b60bcbe821f4c3c69c6",
      "5709e31cb162485282b751c6eac50b23",
      "f82512a620b24a198aa1817322af064d",
      "803e5cb40a8941cfbccf23e043fbe31e",
      "e43ac1663de941e6bbdf7079c20c2872",
      "50e12fe67e8b4b31a9259cd37916e4f7",
      "b35f8dfb74304120b4660d4bd71449e6",
      "ef28908c02414629b6a886031b65123f",
      "8283ab8328934ca49adcec4e7cfb478c",
      "cf77f48ff085427abceb499886857468",
      "d95dffe4dcfd48669d79d4000a3d1562",
      "06bbeb2bda6c47dbbba230937bd088f1",
      "65e5975503214cea9569ac0e5ea51c82",
      "6e2a7b494a044aa380657b1daa832b8e",
      "5b181849c272457b928c3c4e4453faa3",
      "637acb73be6c42eea6804a8a71a65b0f",
      "f5e0017f194949bc99dd9cc869cf6ed3",
      "45029ff8a8b848d8803d614fa9ab6ec8",
      "b243c02184064effb6d557020d719c29",
      "21755833bcab4ca489d383ecf7164a43",
      "f7f044b807a449dcac2835828013779f",
      "f074abedbd9744239c19b39208a816d0",
      "9177858b110f481bae32efcea4d920fd",
      "0db21cbbbb464f96b67664e01faeda59",
      "75cf2e3f60aa4e4999987862183494b9",
      "592b7da71aa44a8fa8a8edfbed23f9ce",
      "5fff32c6a6304af59354b6c1d1e4564e",
      "6391ab5c15294eafa99c6fcf8ad537b3",
      "1c9cc5af82504452afefc11674af69d1",
      "fc1c313a1a824d7196cf4e3b704626b4",
      "99b0ac3a59044cf8a5b01c4910366296",
      "111d022cf5e2401887300063fbfb656c",
      "cc6f3bf2c3004a36a3c8d8e5c2495742",
      "9673318345b2449c8556cc55aeff71e9",
      "114c58ae21774373b06d6391e1c9dd9d",
      "757d124a690044abbdbf5a7c7b75c74c",
      "6e4a1ba62b5146dabfc1ce14d08b7cda",
      "18640a82fe884064a2178f3e702d6a85",
      "e2d67abd85b140b5b14ed5d2126f4cbb",
      "8ce6f342d47345909eb99040223895b7",
      "6ea699f93f1d4522ab57a1d2364e0a72",
      "db6f2c1241c64dfbb99f3c78fc0383c8",
      "03b3fc3362f143838e55c35268387e9a",
      "bf4a41684ab644c4b77170c9aeba4945",
      "a8ba5c20024a4ccb8e323ae7d4b7d01c",
      "6c6603881a164e67b2ea51d15329d2cb",
      "054ce327b7874b1dada68caec6094e29",
      "c4865ed34de6415cbea537e058ded3e2",
      "933f02dd011c42f3bed939329d0f09a9",
      "445418e1a49d47c1bf3f3fa599a14ea1",
      "581f54927ded470f8a399fec5a6db227",
      "36f86eaa95d24f7ba562fdca84811f7c",
      "523fa45b3b6648eda4951e81bb104e78",
      "a34626944a9849c8b35b503774b47298",
      "5294b1c01cc24267b6b3f1a86da0e5d8",
      "40bb0539a028415ba5c3ca74f315b20d",
      "18dbaf63536a457390622fed82879393"
     ]
    },
    "id": "o5lsEvvqmT6t",
    "outputId": "0faffd89-d2b2-47ad-93a4-43055be7bd7a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126be89f06f84aec86a5d8484197f3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe66e69304ea4eb59e6c0ade939f4001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82512a620b24a198aa1817322af064d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2a7b494a044aa380657b1daa832b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cf2e3f60aa4e4999987862183494b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757d124a690044abbdbf5a7c7b75c74c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054ce327b7874b1dada68caec6094e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "GPT2TokenizerFast has no attribute modules",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2682468018>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mdata_collator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataCollatorWithPadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m ppo_trainer = PPOTrainer(\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mppo_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mpolicy_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, processing_class, model, ref_model, reward_model, train_dataset, value_model, data_collator, eval_dataset, optimizers, callbacks, peft_config)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_model\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                 \u001b[0mdisable_dropout_in_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolicyAndValueWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m  \u001b[0;31m# needed for pushing to hub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/utils.py\u001b[0m in \u001b[0;36mdisable_dropout_in_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdisable_dropout_in_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.__class__.__name__} has no attribute {key}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: GPT2TokenizerFast has no attribute modules"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorWithPadding\n",
    "from trl import PPOTrainer, PPOConfig\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Use a standard public model for testing\n",
    "model_name_public = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_public)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(model_name_public).to(device)\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(model_name_public).to(device)\n",
    "reward_model = AutoModelForCausalLM.from_pretrained(model_name_public).to(device) # Placeholder\n",
    "\n",
    "policy_model.train()\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    batch_size=1,\n",
    "    learning_rate=5e-6,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "class TokenizedPromptDataset(Dataset):\n",
    "    def __init__(self, tokenizer, prompts, max_length=64):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompts = prompts\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "    def __getitem__(self, idx):\n",
    "        prompt = self.prompts[idx]\n",
    "        encodings = self.tokenizer(\n",
    "            prompt,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in encodings.items()}\n",
    "        return item\n",
    "\n",
    "prompts = [\"What is AI?\", \"Explain Python:\", \"Describe climate change:\"]\n",
    "train_dataset = TokenizedPromptDataset(tokenizer, prompts)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    ppo_config,\n",
    "    policy_model,\n",
    "    ref_model,\n",
    "    tokenizer,\n",
    "    reward_model=reward_model,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    "    value_model=ref_model\n",
    ")\n",
    "\n",
    "ppo_trainer.train()\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DvdQpL_uN1rW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
