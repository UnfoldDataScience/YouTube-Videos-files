{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "980965db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain[llms]\n",
      "  Downloading langchain-0.0.279-py3-none-any.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 8.6 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "     -------------------------------------- 268.8/268.8 kB 8.3 MB/s eta 0:00:00\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
      "     ---------------------------------------- 7.5/7.5 MB 15.0 MB/s eta 0:00:00\n",
      "Collecting pydantic<3,>=1\n",
      "  Downloading pydantic-2.3.0-py3-none-any.whl (374 kB)\n",
      "     -------------------------------------- 374.5/374.5 kB 5.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from langchain[llms]) (1.23.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from langchain[llms]) (1.4.39)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from langchain[llms]) (2.8.4)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7\n",
      "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from langchain[llms]) (6.0)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from langchain[llms]) (2.28.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from langchain[llms]) (3.8.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from langchain[llms]) (4.0.2)\n",
      "Collecting langsmith<0.1.0,>=0.0.21\n",
      "  Downloading langsmith-0.0.33-py3-none-any.whl (36 kB)\n",
      "Collecting tenacity<9.0.0,>=8.1.0\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Collecting openlm<0.0.6,>=0.0.5\n",
      "  Downloading openlm-0.0.5-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: torch<3,>=1 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from langchain[llms]) (1.13.1)\n",
      "Collecting manifest-ml<0.0.2,>=0.0.1\n",
      "  Downloading manifest_ml-0.0.1-py2.py3-none-any.whl (42 kB)\n",
      "     ---------------------------------------- 42.9/42.9 kB ? eta 0:00:00\n",
      "Collecting clarifai>=9.1.0\n",
      "  Downloading clarifai-9.7.6-py3-none-any.whl (2.4 MB)\n",
      "     ---------------------------------------- 2.4/2.4 MB 12.0 MB/s eta 0:00:00\n",
      "Collecting nlpcloud<2,>=1\n",
      "  Downloading nlpcloud-1.1.44-py3-none-any.whl (4.4 kB)\n",
      "Requirement already satisfied: openai<1,>=0 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from langchain[llms]) (0.26.1)\n",
      "Collecting cohere<5,>=4\n",
      "  Downloading cohere-4.21-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 45.6/45.6 kB ? eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from huggingface-hub) (3.6.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from huggingface-hub) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from huggingface-hub) (4.4.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from huggingface-hub) (2022.11.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from huggingface-hub) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.3-cp39-cp39-win_amd64.whl (266 kB)\n",
      "     -------------------------------------- 266.4/266.4 kB 5.5 MB/s eta 0:00:00\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-win_amd64.whl (3.5 MB)\n",
      "     ---------------------------------------- 3.5/3.5 MB 13.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (1.8.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (22.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (2.0.4)\n",
      "Collecting clarifai-grpc>=9.7.4\n",
      "  Downloading clarifai_grpc-9.7.6-py3-none-any.whl (215 kB)\n",
      "     ------------------------------------- 215.7/215.7 kB 13.7 MB/s eta 0:00:00\n",
      "Collecting tritonclient==2.34.0\n",
      "  Downloading tritonclient-2.34.0-py3-none-any.whl (94 kB)\n",
      "     ---------------------------------------- 94.5/94.5 kB 1.3 MB/s eta 0:00:00\n",
      "Collecting rich==13.4.2\n",
      "  Downloading rich-13.4.2-py3-none-any.whl (239 kB)\n",
      "     ------------------------------------- 239.4/239.4 kB 15.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub) (0.4.5)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "     ---------------------------------------- 87.5/87.5 kB ? eta 0:00:00\n",
      "Collecting pygments<3.0.0,>=2.13.0\n",
      "  Downloading Pygments-2.16.1-py3-none-any.whl (1.2 MB)\n",
      "     ---------------------------------------- 1.2/1.2 MB 10.6 MB/s eta 0:00:00\n",
      "Collecting python-rapidjson>=0.9.1\n",
      "  Downloading python_rapidjson-1.10-cp39-cp39-win_amd64.whl (146 kB)\n",
      "     -------------------------------------- 146.3/146.3 kB 4.4 MB/s eta 0:00:00\n",
      "Collecting backoff<3.0,>=2.0\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from cohere<5,>=4->langchain[llms]) (1.26.12)\n",
      "Collecting fastavro==1.8.2\n",
      "  Downloading fastavro-1.8.2-cp39-cp39-win_amd64.whl (459 kB)\n",
      "     -------------------------------------- 459.7/459.7 kB 7.2 MB/s eta 0:00:00\n",
      "Collecting importlib_metadata<7.0,>=6.0\n",
      "  Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "     ---------------------------------------- 49.4/49.4 kB ? eta 0:00:00\n",
      "Collecting dill>=0.3.5\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "     -------------------------------------- 115.3/115.3 kB 7.0 MB/s eta 0:00:00\n",
      "Collecting sqlitedict>=2.0.0\n",
      "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting redis>=4.3.1\n",
      "  Downloading redis-5.0.0-py3-none-any.whl (250 kB)\n",
      "     -------------------------------------- 250.1/250.1 kB 5.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub) (3.0.9)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Collecting pydantic-core==2.6.3\n",
      "  Downloading pydantic_core-2.6.3-cp39-none-win_amd64.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 13.9 MB/s eta 0:00:00\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.5.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain[llms]) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain[llms]) (2022.9.24)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain[llms]) (1.1.3)\n",
      "Requirement already satisfied: protobuf>=3.20.3 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from clarifai-grpc>=9.7.4->clarifai>=9.1.0->langchain[llms]) (4.23.3)\n",
      "Requirement already satisfied: googleapis-common-protos>=1.53.0 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from clarifai-grpc>=9.7.4->clarifai>=9.1.0->langchain[llms]) (1.56.4)\n",
      "Requirement already satisfied: grpcio>=1.44.0 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from clarifai-grpc>=9.7.4->clarifai>=9.1.0->langchain[llms]) (1.54.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from importlib_metadata<7.0,>=6.0->cohere<5,>=4->langchain[llms]) (3.8.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain[llms]) (0.4.3)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: sqlitedict\n",
      "  Building wheel for sqlitedict (setup.py): started\n",
      "  Building wheel for sqlitedict (setup.py): finished with status 'done'\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=1466d0a900319353b65264629147be20bb06ae681662dbb030479ab4275172d8\n",
      "  Stored in directory: c:\\users\\amanr\\appdata\\local\\pip\\cache\\wheels\\f6\\48\\c4\\942f7a1d556fddd2348cb9ac262f251873dfd8a39afec5678e\n",
      "Successfully built sqlitedict\n",
      "Installing collected packages: tokenizers, sqlitedict, safetensors, typing-extensions, tenacity, redis, python-rapidjson, pygments, mdurl, importlib_metadata, fastavro, dill, backoff, annotated-types, typing-inspect, tritonclient, pydantic-core, openlm, nlpcloud, marshmallow, markdown-it-py, manifest-ml, huggingface-hub, clarifai-grpc, transformers, rich, pydantic, dataclasses-json, cohere, langsmith, clarifai, langchain\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.0.1\n",
      "    Uninstalling tenacity-8.0.1:\n",
      "      Successfully uninstalled tenacity-8.0.1\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.11.2\n",
      "    Uninstalling Pygments-2.11.2:\n",
      "      Successfully uninstalled Pygments-2.11.2\n",
      "  Attempting uninstall: importlib_metadata\n",
      "    Found existing installation: importlib-metadata 4.11.3\n",
      "    Uninstalling importlib-metadata-4.11.3:\n",
      "      Successfully uninstalled importlib-metadata-4.11.3\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.4\n",
      "    Uninstalling dill-0.3.4:\n",
      "      Successfully uninstalled dill-0.3.4\n",
      "Successfully installed annotated-types-0.5.0 backoff-2.2.1 clarifai-9.7.6 clarifai-grpc-9.7.6 cohere-4.21 dataclasses-json-0.5.14 dill-0.3.7 fastavro-1.8.2 huggingface-hub-0.16.4 importlib_metadata-6.8.0 langchain-0.0.279 langsmith-0.0.33 manifest-ml-0.0.1 markdown-it-py-3.0.0 marshmallow-3.20.1 mdurl-0.1.2 nlpcloud-1.1.44 openlm-0.0.5 pydantic-2.3.0 pydantic-core-2.6.3 pygments-2.16.1 python-rapidjson-1.10 redis-5.0.0 rich-13.4.2 safetensors-0.3.3 sqlitedict-2.1.0 tenacity-8.2.3 tokenizers-0.13.3 transformers-4.32.1 tritonclient-2.34.0 typing-extensions-4.7.1 typing-inspect-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.2.2 requires pyqt5<5.13, which is not installed.\n",
      "spyder 5.2.2 requires pyqtwebengine<5.13, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain[llms] huggingface-hub langchain transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdc23da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"E:\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a99808e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d453a6751c43bebf638be99740d1f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/6.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amanr\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\amanr\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amanr\\.cache\\huggingface\\hub\\models--lmsys--fastchat-t5-3b-v1.0\\snapshots\\0b1da230a891854102d749b93f7ddf1f18a81024\\pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b49c58dd5c94c07a8e0097fc1db70a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/added_tokens.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amanr\\.cache\\huggingface\\hub\\models--lmsys--fastchat-t5-3b-v1.0\\snapshots\\0b1da230a891854102d749b93f7ddf1f18a81024\\added_tokens.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93db50f0cf294222bae562225aa616ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amanr\\.cache\\huggingface\\hub\\models--lmsys--fastchat-t5-3b-v1.0\\snapshots\\0b1da230a891854102d749b93f7ddf1f18a81024\\config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f6b8a6c4e7465cb3f108c6b6422bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amanr\\.cache\\huggingface\\hub\\models--lmsys--fastchat-t5-3b-v1.0\\snapshots\\0b1da230a891854102d749b93f7ddf1f18a81024\\generation_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494a91370e79437fb570d7d93abceda1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amanr\\.cache\\huggingface\\hub\\models--lmsys--fastchat-t5-3b-v1.0\\snapshots\\0b1da230a891854102d749b93f7ddf1f18a81024\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788c7a9c657a43eaa1193cf99b22a45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amanr\\.cache\\huggingface\\hub\\models--lmsys--fastchat-t5-3b-v1.0\\snapshots\\0b1da230a891854102d749b93f7ddf1f18a81024\\spiece.model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45650ef2eda345c69704a54a86d1d3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amanr\\.cache\\huggingface\\hub\\models--lmsys--fastchat-t5-3b-v1.0\\snapshots\\0b1da230a891854102d749b93f7ddf1f18a81024\\tokenizer_config.json\n",
      "C:\\Users\\amanr\\.cache\\huggingface\\hub\\models--lmsys--fastchat-t5-3b-v1.0\\snapshots\\0b1da230a891854102d749b93f7ddf1f18a81024\\tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "#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\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "HUGGING_FACE_API_KEY = \"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\"\n",
    "\n",
    "# Replace this if you want to use a different model\n",
    "model_id = \"lmsys/fastchat-t5-3b-v1.0\"\n",
    "filenames = [\"pytorch_model.bin\", \"added_tokens.json\", \"config.json\", \"generation_config.json\",\n",
    "    \"special_tokens_map.json\", \"spiece.model\", \"tokenizer_config.json\"\n",
    "]\n",
    "\n",
    "for filename in filenames:\n",
    "    downloaded_model_path = hf_hub_download(\n",
    "        repo_id=model_id,\n",
    "        filename=filename,\n",
    "        token=HUGGING_FACE_API_KEY\n",
    "    )\n",
    "\n",
    "    print(downloaded_model_path)\n",
    "\n",
    "print(downloaded_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7391c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e09163a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10772\\2611211232.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"lmsys/fastchat-t5-3b-v1.0\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m llm = HuggingFacePipeline.from_model_id(\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mmodel_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"text2text-generation\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"temperature\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"max_length\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\llms\\huggingface_pipeline.py\u001b[0m in \u001b[0;36mfrom_model_id\u001b[1;34m(cls, model_id, task, device, model_kwargs, pipeline_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0m_model_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_kwargs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0m_model_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m                     \u001b[1;34mf\"Tokenizer class {tokenizer_class_candidate} does not exist or is not currently imported.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m                 )\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m         \u001b[1;31m# Otherwise we have to be creative.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1852\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1854\u001b[1;33m         return cls._from_pretrained(\n\u001b[0m\u001b[0;32m   1855\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1856\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2015\u001b[0m         \u001b[1;31m# Instantiate tokenizer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2016\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2017\u001b[1;33m             \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2018\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2019\u001b[0m             raise OSError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vocab_file, tokenizer_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m                 )\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         super().__init__(\n\u001b[0m\u001b[0;32m    134\u001b[0m             \u001b[0mvocab_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mtokenizer_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenizer_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mfast_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_slow_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslow_tokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    121\u001b[0m                 \u001b[1;34m\"Couldn't instantiate the backend tokenizer from one of: \\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                 \u001b[1;34m\"(1) a `tokenizers` library serialization file, \\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."
     ]
    }
   ],
   "source": [
    "model_id = \"lmsys/fastchat-t5-3b-v1.0\"\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id,\n",
    "    task=\"text2text-generation\",\n",
    "    model_kwargs={\"temperature\": 0, \"max_length\": 1000}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a6b3172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp39-cp39-win_amd64.whl (977 kB)\n",
      "     ------------------------------------- 977.6/977.6 kB 10.3 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "903eff24",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20248\\815048949.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprompt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPromptTemplate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_variables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"question\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mllm_chain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLLMChain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mllm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mllm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "You are a friendly chatbot assistant that responds conversationally to users' questions.\n",
    "Keep the answers short, unless specifically asked by the user to elaborate on something.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef337ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question):\n",
    "    result = llm_chain(question)\n",
    "    print(result['question'])\n",
    "    print(\"\")\n",
    "    print(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4e10d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class TimerError(Exception):\n",
    "    \"\"\"A custom exception used to report errors in use of Timer class\"\"\"\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self._start_time = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        if self._start_time is not None:\n",
    "            raise TimerError(f\"Timer is running. Use .stop() to stop it\")\n",
    "        self._start_time = time.perf_counter()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self._start_time is None:\n",
    "            raise TimerError(f\"Timer is not running. Use .start() to start it\")\n",
    "        elapsed_time = time.perf_counter() - self._start_time\n",
    "        self._start_time = None\n",
    "        print(f\"Elapsed time: {elapsed_time:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb848cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer():\n",
    "    ask_question(\"Describe some famous landmarks in London\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
