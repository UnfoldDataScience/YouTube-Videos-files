{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b91ca89-2c63-4434-91b3-dca0fcc9ca5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the .env file from local\n",
    "from dotenv import load_dotenv\n",
    "env_path = r'E:\\YTReusable\\.env'\n",
    "load_dotenv(env_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dce710-b064-44a4-8309-cd62a2e5951c",
   "metadata": {},
   "source": [
    "# Google Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2796980-2da5-47da-97ad-2f8e9a505bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI works by mimicking human intelligence processes through machines, particularly computer systems.  It doesn't actually \"think\" like a human, but it can perform tasks that typically require human intelligence.  This is achieved through a combination of different techniques, but broadly falls under these categories:\n",
      "\n",
      "**1. Machine Learning (ML):** This is the most prevalent approach.  Instead of explicitly programming a computer with rules, ML algorithms learn from data.  They identify patterns, make predictions, and improve their accuracy over time based on the data they're exposed to.  There are several types of ML:\n",
      "\n",
      "* **Supervised Learning:** The algorithm is trained on a labeled dataset (data with known inputs and outputs).  It learns to map inputs to outputs and then predicts outputs for new, unseen inputs.  Examples include image classification (identifying cats vs. dogs) and spam detection.\n",
      "* **Unsupervised Learning:** The algorithm is trained on an unlabeled dataset.  It identifies patterns and structures in the data without explicit guidance.  Examples include clustering (grouping similar data points together) and dimensionality reduction (reducing the number of variables while preserving important information).\n",
      "* **Reinforcement Learning:** The algorithm learns through trial and error.  It interacts with an environment, receives rewards or penalties for its actions, and learns to maximize its cumulative reward.  Examples include game playing (e.g., AlphaGo) and robotics.\n",
      "\n",
      "**2. Deep Learning (DL):** A subset of ML that uses artificial neural networks with multiple layers (hence \"deep\").  These networks are inspired by the structure and function of the human brain.  They excel at processing complex, unstructured data like images, audio, and text.  Examples include image recognition, natural language processing (NLP), and speech recognition.\n",
      "\n",
      "**3. Natural Language Processing (NLP):**  Focuses on enabling computers to understand, interpret, and generate human language. This involves tasks like:\n",
      "\n",
      "* **Text Classification:** Categorizing text into predefined categories (e.g., sentiment analysis – positive, negative, neutral).\n",
      "* **Machine Translation:** Translating text from one language to another.\n",
      "* **Text Summarization:** Condensing large amounts of text into shorter summaries.\n",
      "* **Chatbots:**  Interacting with humans through conversational interfaces.\n",
      "\n",
      "**4. Computer Vision:**  Enables computers to \"see\" and interpret images and videos.  This involves tasks like:\n",
      "\n",
      "* **Object Detection:** Identifying and locating objects within an image or video.\n",
      "* **Image Segmentation:** Partitioning an image into meaningful regions.\n",
      "* **Image Recognition:** Classifying images into different categories.\n",
      "\n",
      "**How it all works together:**  Often, multiple AI techniques are combined to create complex systems.  For example, a self-driving car might use computer vision to perceive its surroundings, deep learning to process the visual data, and reinforcement learning to learn optimal driving strategies.\n",
      "\n",
      "**Limitations:**  AI systems are not without limitations. They can be biased if trained on biased data, they can struggle with tasks that require common sense reasoning or creativity, and they can be vulnerable to adversarial attacks (deliberately crafted inputs designed to fool the system).  Furthermore, ethical considerations surrounding AI's impact on society are increasingly important.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!pip install google.generativeai\n",
    "#Good for API based work\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure()\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "response = model.generate_content(\"Explain how AI works\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ab0cf7-07fc-48e6-adb2-e61aa2cc0e0a",
   "metadata": {},
   "source": [
    "# Huggingface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87726251-f412-45d1-88ef-f81a1093adc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af7f752f-a3d5-4b49-b83c-a4d5e480d983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=100)\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffe9eb07-2f5e-4833-9779-0cb0d7af1235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is electroencephalography?\n",
      "\n",
      "Answer: Let's think step by step. It's a lot easier to understand in terms of what the brain uses when it is activated than it is to understand in terms of what happens when they are placed in that place.\n",
      "\n",
      "In terms of brain activity it's just something that your mind can absorb, so you don't have to think about it and it's pretty nice, but in this case we actually need the brain to work here so there shouldn't be any interference into the brain. We need the brain to work and it needs\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade langchain langchain_core\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | hf\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "print(chain.invoke({\"question\": question}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e812df-2bb2-45f4-83d7-727aacfc750c",
   "metadata": {},
   "source": [
    "# GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a10e7916-0d77-42ba-b7ff-b960b00f8c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gpt4all\n",
      "  Downloading gpt4all-2.8.2-py3-none-win_amd64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from gpt4all) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from gpt4all) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from requests->gpt4all) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from requests->gpt4all) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from requests->gpt4all) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from requests->gpt4all) (2024.12.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\amanr\\anaconda3\\lib\\site-packages (from tqdm->gpt4all) (0.4.6)\n",
      "Downloading gpt4all-2.8.2-py3-none-win_amd64.whl (119.6 MB)\n",
      "   ---------------------------------------- 0.0/119.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.8/119.6 MB 10.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 4.5/119.6 MB 11.2 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 7.1/119.6 MB 11.8 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 9.7/119.6 MB 12.3 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 13.4/119.6 MB 13.1 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 17.0/119.6 MB 13.9 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 20.4/119.6 MB 14.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 23.6/119.6 MB 14.5 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 26.2/119.6 MB 14.2 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 28.8/119.6 MB 14.0 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 31.5/119.6 MB 13.9 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 33.6/119.6 MB 13.7 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 35.7/119.6 MB 13.3 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 37.7/119.6 MB 13.1 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 39.1/119.6 MB 12.7 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 40.6/119.6 MB 12.3 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 41.7/119.6 MB 12.0 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 42.7/119.6 MB 11.7 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 43.5/119.6 MB 11.3 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 44.3/119.6 MB 10.8 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 45.1/119.6 MB 10.5 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 46.1/119.6 MB 10.2 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 47.2/119.6 MB 9.9 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 48.2/119.6 MB 9.7 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 49.3/119.6 MB 9.6 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 50.3/119.6 MB 9.4 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 51.4/119.6 MB 9.2 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 52.4/119.6 MB 9.0 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 53.7/119.6 MB 8.9 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 55.1/119.6 MB 8.9 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 56.1/119.6 MB 8.8 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 57.1/119.6 MB 8.6 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 57.9/119.6 MB 8.5 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 59.2/119.6 MB 8.4 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 60.6/119.6 MB 8.3 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 62.1/119.6 MB 8.3 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 63.7/119.6 MB 8.3 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 65.0/119.6 MB 8.2 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 65.8/119.6 MB 8.1 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 66.6/119.6 MB 8.0 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 67.6/119.6 MB 7.9 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 68.4/119.6 MB 7.8 MB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 69.2/119.6 MB 7.7 MB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 70.3/119.6 MB 7.6 MB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 71.0/119.6 MB 7.6 MB/s eta 0:00:07\n",
      "   ------------------------ --------------- 71.8/119.6 MB 7.5 MB/s eta 0:00:07\n",
      "   ------------------------ --------------- 72.6/119.6 MB 7.4 MB/s eta 0:00:07\n",
      "   ------------------------ --------------- 73.7/119.6 MB 7.3 MB/s eta 0:00:07\n",
      "   ------------------------ --------------- 74.7/119.6 MB 7.3 MB/s eta 0:00:07\n",
      "   ------------------------- -------------- 75.8/119.6 MB 7.2 MB/s eta 0:00:07\n",
      "   ------------------------- -------------- 76.8/119.6 MB 7.2 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 77.6/119.6 MB 7.1 MB/s eta 0:00:06\n",
      "   -------------------------- ------------- 78.6/119.6 MB 7.1 MB/s eta 0:00:06\n",
      "   -------------------------- ------------- 80.2/119.6 MB 7.1 MB/s eta 0:00:06\n",
      "   --------------------------- ------------ 81.5/119.6 MB 7.1 MB/s eta 0:00:06\n",
      "   --------------------------- ------------ 83.1/119.6 MB 7.1 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 84.7/119.6 MB 7.1 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 86.5/119.6 MB 7.1 MB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 87.3/119.6 MB 7.1 MB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 88.6/119.6 MB 7.0 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 89.9/119.6 MB 7.0 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 91.5/119.6 MB 7.0 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 93.3/119.6 MB 7.0 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 94.9/119.6 MB 7.1 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 96.5/119.6 MB 7.1 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 98.0/119.6 MB 7.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 99.9/119.6 MB 7.1 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 102.0/119.6 MB 7.1 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 104.3/119.6 MB 7.2 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 106.2/119.6 MB 7.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 107.7/119.6 MB 7.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 109.8/119.6 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 112.7/119.6 MB 7.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 115.3/119.6 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  116.7/119.6 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  119.0/119.6 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  119.5/119.6 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 119.6/119.6 MB 7.3 MB/s eta 0:00:00\n",
      "Installing collected packages: gpt4all\n",
      "Successfully installed gpt4all-2.8.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\amanr\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\amanr\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\amanr\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "! pip install gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a652112-ff7c-48a7-93ce-ea8b87c663ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|███████████████████████████████████████████████████████████████| 2.18G/2.18G [02:05<00:00, 17.4MiB/s]\n",
      "Verifying: 100%|██████████████████████████████████████████████████████████████████| 2.18G/2.18G [00:05<00:00, 368MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To run large language models (LLMs) like GPT-3 or similar efficient versions of such models, you'll need to consider a few factors due to their computational requirements. Here are some steps and tips for running them more effectively on your laptop:\n",
      "\n",
      "1. Choose an optimized model version: There are smaller LLM variants available that have been trained specifically with efficiency in mind (e.g., DistilGPT, TinyBERT). These models can run faster while still providing good performance compared to their larger counterparts.\n",
      "\n",
      "2. Use efficient hardware acceleration techniques: \n",
      "   - GPUs: If your laptop has a dedicated NVIDIA or AMD graphics card with CUDA/OpenCL support (e.g., GeForce RTX, Quadro series), you can leverage the power of these devices to accelerate LLM computations using libraries like TensorFlow or PyTorch.\n",
      "   - CPUs: If your laptop doesn't have a dedicated GPU but has multiple cores and high clock speeds in its processors (e.g., Intel i7/i9, AMD Ryzen), you can still benefit from parallel processing to speed up the computations by using libraries like Dask or Ray that enable distributed computing on CPUs.\n",
      "   - Optimize your code: Use optimized data structures and algorithms for loading input data into memory efficiently (e.g., use PyTorch's DataLoader with batching, caching, etc.) to reduce I/O bottlenecks during model inference or training tasks.\n",
      "\n",
      "3. Reduce the size of LLM models when possible: Some smaller versions of popular language models like BERT and GPT-2 are available (e.g., DistilBERT, TinyGPT). These can provide a good balance between performance and resource requirements for your laptop's capabilities.\n",
      "\n",
      "4. Use model quantization techniques to reduce memory footprint: Quantizing the weights of an LLM reduces its size without significantly affecting accuracy. This technique helps in deploying models on devices with limited resources, like laptops or mobile phones. You can use libraries such as TensorFlow's tfq module and PyTorch's torch.quantization to apply quantization techniques easily.\n",
      "\n",
      "5. Implement model pruning: Prune the less important connections in an LLM by removing weights with small values, which reduces its size without significantly affecting performance (e.g., using FLAX library). This can help make it more suitable for running on a laptop's limited resources.\n",
      "\n",
      "6. Optimize batch sizes and sequence lengths: Adjust the input data to match your device's memory capacity by reducing batch sizes or truncating long sequences, while still maintaining an acceptable level of performance in tasks like text generation. This can help reduce resource usage during inference time on a laptop.\n",
      "\n",
      "7. Use cloud-based solutions for heavy computations: If you need even more computational power than your laptop's hardware allows and want to avoid local processing limitations entirely, consider using remote servers or the cloud (e.g., Google Colab) with access to powerful GPU instances that can handle larger LLM models efficiently while running on a smaller scale locally.\n",
      "\n",
      "Remember that each of these techniques comes with trade-offs in terms of model performance and resource usage; you'll need to experiment with different approaches based on your specific use case, available hardware resources, and desired accuracy levels for the task at hand.\n"
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "model = GPT4All(\"Phi-3-mini-4k-instruct.Q4_0.gguf\") # downloads / loads a 4.66GB LLM\n",
    "with model.chat_session():\n",
    "    print(model.generate(\"How can I run LLMs efficiently on my laptop?\", max_tokens=1024))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48327239-ac47-42b5-8bf6-bead15c3c490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pound fool\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(\"Please complete the proverb - penny wise ---\", max_tokens=3)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a374b89-7a26-4f9e-bc35-1554ec83128c",
   "metadata": {},
   "source": [
    "# ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70586aee-d59b-4507-8876-e13f80a4dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d10e42c9-efdd-4687-8dff-ae3423429b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "DSmodel = OllamaLLM(model=\"deepseek-r1:1.5b\", base_url=\"http://127.0.0.1:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ea918f5-1711-4591-8233-72c5d8de3e43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nAlright, I need to write a concise answer of about 100-200 words on India. The example provided is quite well-structured, breaking down the key points with clear headings.\\n\\nFirst, I should introduce India as an ancient country in South Asia known for its diversity and cultural richness. Then mention its historical significance, highlighting key cities like Delhi and Mumbai, which are iconic. Next, emphasize its diversity—diverse cultures, languages, religions, and landscapes. Highlighting unique aspects of each state could add depth.\\n\\nThe example talks about geography, climate, natural beauty, cuisine, and festivals as essential parts of India's landscape. That makes sense. Including how people live in cities versus rural areas is important too.\\n\\nI need to ensure the language is clear and concise, avoiding any unnecessary jargon. The answer should be actionable, so maybe include a tip on visiting important places or exploring cultural spots like temples or museums.\\n\\nI should also keep it under 200 words, so I have to be efficient with my information without losing key points. Maybe check the word count after drafting to adjust if needed.\\n\\nThinking about structure: Introduction, history, diversity and culture, geography and natural beauty, daily life in cities vs rural areas, tips for exploring. That seems logical.\\n\\nI should also make sure each section flows smoothly into the next, maintaining a reader-friendly tone. Using bullet points or bold headings might help with clarity, but since it's an answer, maybe just sections without them.\\n\\nLet me outline the key points to cover:\\n\\n1. Introduction: Ancient country in South Asia, diversity.\\n2. Key cities (Delhi, Mumbai) and their significance.\\n3. Diversity across states—cultures, languages, religions, landscapes.\\n4. Natural beauty aspects like mountains, rivers, forests.\\n5. Unique cuisines and festivals.\\n6. Daily life structures: urban vs rural.\\n\\nNow, organizing these points in a coherent manner without exceeding the word limit. I need to ensure each point is clear and contributes to the overall understanding of India as presented in the answer.\\n</think>\\n\\nIndia, known for its ancient history and vibrant diversity, has captivated generations worldwide. Renowned for its diverse cultures and languages, it’s home to cities like Delhi and Mumbai, symbolizing the world's journey through varied traditions. The country boasts a rich tapestry of histories, religions, and landscapes, from towering peaks to serene forests.\\n\\nIndia's natural beauty is unparalleled, with mountains, rivers, and dense forests providing the essence of its landscape. Cuisines from across regions showcase the charm of Indian cuisine, while festivals like Diwali and Diya highlight cultural richness. Daily life varies—rural areas are quiet, while cities thrive with bustling streets.\\n\\nTo explore India effectively, consider visiting iconic places like Dehrington Hill for mountain views or serene temples like Bsprintfala in Tamil Nadu. This journey through diverse traditions, cultures, and landscapes will enrich your understanding of India's legacy.\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt template\n",
    "template = \"\"\"Question: {question} \n",
    "Answer: Let's keep it concise and actionable under 200 words\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = prompt | DSmodel\n",
    "\n",
    "chain.invoke({\"question\": \"Please tell me about India in 100 words\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300723cc-b0ac-49be-8c8c-3277a97cbe88",
   "metadata": {},
   "source": [
    "# Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e41db0bf-9545-4515-bf79-7c43bd5541d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4e0ad1d-6d80-4cb6-8caa-00c9d14a8e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a 100-word cold email:\n",
      "\n",
      "Subject: Exploring EdTech Collaborations\n",
      "\n",
      "Hi [Name],\n",
      "I'm [Your Name], a UK-based data scientist and founder of Unfold Data Science (100k+ YouTube subscribers). I'm eager to connect with EdTech professionals and explore potential collaborations, particularly in training. Let's discuss how we can work together to create impactful learning experiences. Would you be open to a brief call?\n",
      "Best, [Your Name]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Please act like a mentor of communication and networking,\"\n",
    "            \"help me write a small cold email. About me and context below:\"\n",
    "            \"I am a data scientist based out of UK running a youtube channel Unfold data science\"\n",
    "            \"where 100k people learn data science from me. \"\n",
    "            \"My intent is to network and get connected with people in Ed tech industry\"\n",
    "            \"to explore potential collaboration expecially in training domain. Please keep within 100 words\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d465cb-5714-48ad-8164-139f50d3de03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
